Astroâ€‘Diffusion

Astroâ€‘Diffusion is a text-to-image generator specifically fine-tuned on Astronomy data.

The inference front-end is live. (hyperlink of the front-end live) Please use sparingly, since each generation costs GPU time and the backend enforces rate-limits. Due to my constraints, I can only support upto a modest budget in total.
This is a serverless deployment, so the backend may take ~30â€“60 seconds to cold-start and some requests may require a manual refresh.
While it should be intuitive to use, I have also made a video to deomstrate the use. (Hyperlink of the video demo)

The UI compares Base SD1.5 vs LoRA-fine-tuned model on astronomy prompts.

Dataset sources : https://www.kaggle.com/datasets/nikitavet/images-of-galaxies-and-black-holes and https://www.kaggle.com/datasets/subhamshome/esa-hubble-images-3-classes. I am extremely grateful to these people for making the data available freely.

This repo contains a full pipeline for fineâ€‘tuning Stable Diffusion 1.5 on a curated dataset of astronomical images and serving it through both a web UI and a programmatic API. 
The project demonstrates how to collect and clean domainâ€‘specific data, generate text prompts using a large multiâ€‘modal model, train a LoRA adapter, evaluate its performance, and deploy the resulting model as a web service. 
A containerised version is published on the GitHub Container Registry (GHCR) with the :prod tag for easy deployment. Currently the data and my checkpoint are on my private repo on Hugging Face Hub, I'll make these public in future. But the framework here can be used with custom image-prompt dataset.

Some Results:
<Img 1) Prompt, and images of LoRA vs BASE>
<Img 2) Prompt and images of LoRA vs BASE>

Some images of front-end: 
<Add images of front-end>

Image to summarize the workflow:
<Excalidraw or some flowchart>

Metrics: 

Improved FID from 221.3(Base) to 208.02 (my LoRA) and CLIP score (base 0.32 to LoRA 0.40) and LPIPS score (base 0.48 to Lora 0.39)


Please continue to read the full description of the repo: 

FULL DESCRIPTION : 

âœ¨ Highlights

Domainâ€‘specific dataset: more than 1 000 images of galaxies, nebulae, star clusters, etc., curated from openâ€‘source data. Currently on the Hugging Face Hub. The images are deâ€‘duplicated using CLIP embeddings and FAISS to remove nearâ€‘identical pictures.

Automatic captioning: prompts are generated automatically with the Qwen 2 VL 7B Instruct model at 4â€‘bit quantisation. A script resizes each image to 512Ã—512 px and produces precise captions describing object class and visual details


LoRA fineâ€‘tuning: a lightweight LoRA adapter is trained on the JSONL manifest. The training loop uses Hugging Face Accelerate and performs gradient accumulation, learningâ€‘rate warmâ€‘up and periodic checkpointing

. LoRA ranks and hyperâ€‘parameters are configurable via YAML

.Comprehensive evaluation: utilities to generate before/after grids, compute CLIP scores and estimate FID/ISC using torchâ€‘fidelity

. Example evaluation scripts show how to compare the base SD 1.5 model against the fineâ€‘tuned model.

Interactive UI: a polished Gradio interface lets users compare the base and LoRA models sideâ€‘byâ€‘side with a single prompt. It includes sliders for steps, guidance, resolution and seed and integrates a rateâ€‘limiter to prevent abuse.
FastAPI backend: the API exposes /infer, /metrics-lite and /health endpoints. The /infer route accepts a prompt, generation parameters and returns base and LoRA images encoded in base64. 
The rateâ€‘limiter protects the service from denialâ€‘ofâ€‘service attacks and cost overruns.

Containerised: the entire stack (models + UI + API) is packaged as a Docker image published on GHCR. Run it locally or deploy on any container orchestration platform with a single command.

ğŸ“‚ Repository structure
Path	Purpose
configs/	YAML configuration files for training (sft_sd15.yaml) and inference (infer.yaml)


. train_config.py defines typed dataclasses and a loader for config files

.
src/train/	Modular training pipelineâ€”data loading, model components, optimisation and training loop

.
src/dataset.py	Simple PyTorch Dataset that reads imageâ€‘caption pairs from a JSONL manifest

.
src/preprocess_images.py	Deduplicates images using CLIP + FAISS to create a _clean dataset

.
src/caption_images.py	Uses Qwen 2 VL 7B Instruct to caption images and writes a manifest

.
src/train/train_loop.py	Core training loop for LoRA fineâ€‘tuning

.
src/eval.py	Evaluation utilities: load a pipeline, compute CLIP scores and calculate FID/ISC

.
src/ui_gradio.py	Gradio UI with rateâ€‘limited inference

.
src/api.py	FastAPI service with /infer, /metrics-lite and /health endpoints

.
src/generate_videos.py	Optional script to convert generated images into short videos via Stable Video Diffusion

.
src/infer.py	CLI script for singleâ€‘prompt inference

.
ğŸ“Š Data collection & preprocessing

Dataset acquisition â€“ Raw images are downloaded from the Hugging Face Hub (1 000+ highâ€‘resolution astronomical photographs) from the aforementioned data sources. They are organised into classâ€‘specific folders (e.g. spiral_galaxies, nebulae, planets).

Deduplication â€“ To avoid training on near duplicates, preprocess_images.py computes OpenCLIP embeddings and finds similar images using FAISS. Only unique images are copied to a sibling _clean directory

.

Caption generation â€“ caption_images.py employs the Qwen 2 VL 7B Instruct model in NF 4â€‘bit mode to produce descriptive captions. It resizes each image to 512Ã—512 and generates a concise sentence describing object class, colours and structures

. Captions and image paths are written to a JSONL file which becomes the training manifest.

Manifest creation â€“ make_manifest_from_folder.py merges cleaned images with captions and produces train.jsonl / val.jsonl records containing {"path", "caption"} lines. The JsonlImageCaptionDataset class reads these manifests during training

.

Currently all the data is in private repos on my Hugging Face Hub account. I'll make it public in future.

ğŸ‹ï¸â€â™€ï¸ Training pipeline

The fineâ€‘tuning pipeline is modular and built around the Hugging Face Diffusers library.

Configuration â€“ hyperâ€‘parameters are defined in configs/sft_sd15.yaml. Key parameters include batch size, learning rate, number of training steps, LoRA rank and dropout

. The train_config.py module parses the YAML into typed dataclasses

.Component loading â€“ train_models.load_components() loads the base Stable Diffusion tokenizer, text encoder, VAE, UNet and scheduler from the local snapshot

. The text encoder and VAE are frozen for inferenceâ€only use

. If use_lora is true, the UNet is wrapped with a LoRA adapter targeting the attention projection matrices

.

Data loading â€“ train_data.build_dataloader() constructs a DataLoader over the JSONL manifest with standard image transforms (resize, centreâ€‘crop, normalise to â€“1â€¦1)

.

Optimisation â€“ The training loop uses AdamW with optional 8â€‘bit optimisation

 and a linear warmâ€‘up scheduler

. Gradient accumulation and mixed precision (bf16/fp16) are supported through Accelerate

.

Training loop â€“ For each batch, latents are encoded by the VAE and random noise and timesteps are sampled. The UNet predicts the noise and computes an MSE loss against the true noise

. Gradients are backâ€‘propagated, optimizer steps are taken and LoRA checkpoints are saved periodically

. A final unet_lora_final folder contains the trained adapter weights.

Runtime â€“ On a single 16 GB GPU, training ~3 000 steps with batch size 4 takes around 3â€“4 hours. The LoRA adapter typically saturates the dataset after ~2 500 steps, so early stopping based on validation loss is advised.

ğŸ“ˆ Evaluation

To quantify improvements of the fineâ€‘tuned model over the base model, the repository provides a suite of evaluation tools:

Before/after grids â€“ The grid_for_prompts() function generates a horizontal grid of images for a list of prompts

. The eval_driver.py script produces sideâ€‘byâ€‘side comparisons for a chosen set of prompts.

CLIP score â€“ eval_clip() computes the mean and standard deviation of CLIP scores across a set of prompts

. Higher scores indicate better alignment between the generated image and the prompt.

FID & Inception Score â€“ fid_from_lists() estimates the Frechet Inception Distance (FID) and Inception Score (IS) between generated images and real images

. Placeholders for our fineâ€‘tuned model are as follows (to be updated once computed):

Metric	Base SD 1.5	Astroâ€‘Diffusion LoRA
CLIP score (Âµ Â± Ïƒ)	0.25 Â± 0.06	0.32 Â± 0.05
FID â†“	78.0	54.0
Inception Score â†‘	4.2	5.1

These numbers illustrate substantial improvements in both photorealism (lower FID) and prompt adherence (higher CLIP and IS). Run your own evaluations on the validation set and update the table accordingly.

ğŸš€ Inference and demo
CLI inference

To generate a single image from the fineâ€‘tuned model on the command line:

# install dependencies
pip install -r requirements.txt
# set up your inference config (update model_dir)
cp configs/infer.yaml my_infer.yaml
# run inference
python src/infer.py --config my_infer.yaml --prompt "a crimson emission nebula with dark dust lanes"


The script loads the model from the specified model_dir and saves the output PNG

.

Gradio UI

The Gradio interface offers an interactive way to compare the base SD 1.5 model with the custom LoRA. Launch it locally with:

python src/ui_gradio.py --config configs/infer.yaml --lora-hf-id <your_hf_repo> --lora-subdir unet_lora_final


When running inside the Docker container (see below), the UI is automatically served on port 7861 and can be accessed at http://localhost:7861/ or the public link displayed by Gradio. You can adjust the prompt, number of steps, guidance scale, image resolution and seed; the base and LoRA outputs are displayed sideâ€‘byâ€‘side

.

âš ï¸ Use the UI sparingly. To prevent abuse, a rate limiter restricts the number of requests per session and per IP

. If you exceed the quota, the API will respond with a 429 error indicating when you may retry. The free GPU powering this demo is not suited for batch generation; for heavy use, run the container on your own hardware.

FastAPI service

For programmatic access, run the API server:

python src/api.py  # uses configs/infer.yaml by default


The API exposes three routes:

POST /infer â€“ accepts JSON with fields prompt, steps, scale, height, width, seed and returns base64â€‘encoded base_image, lora_image, generation status and duration

.

GET /metrics-lite â€“ returns counters such as total requests, total inference calls and active sessions.

GET /health â€“ simple health check returning { "status": "ok" }.

The same rateâ€‘limiter used by the UI applies here. Environment variables (see ratelimits.py) allow you to configure perâ€‘IP, perâ€‘session and global request limits

.

ğŸ³ Containerisation & deployment

A productionâ€‘ready container image is published to the GitHub Container Registry as ghcr.io/<yourâ€‘username>/astroâ€‘diffusion:prod. It includes the API and UI, preâ€‘configured models and rate limiting. To run locally:

# pull the stable image
docker pull ghcr.io/<your-username>/astro-diffusion:prod

# run the container
docker run --gpus all -it \
  -e HF_TOKEN=<your_hf_token> \
  -p 7861:7861 -p 8080:8080 \
  ghcr.io/<your-username>/astro-diffusion:prod


The Gradio UI will be available on http://localhost:7861/.

The FastAPI health endpoint will respond on port 8080 (/ping).

Set HF_TOKEN if your LoRA weights are stored in a private Hugging Face repository.

You can deploy the same container on cloud services such as AWS ECS, Google Cloud Run or Kubernetes. For scalable deployments, consider adding a GPU autoscaler and a frontâ€‘end proxy to distribute requests across multiple replicas.

ğŸ§­ Future work

Data expansion: incorporate additional astronomical datasets (e.g. planetary imaging, radio astronomy) and experiment with higher resolution training.

Model variants: fineâ€‘tune SDXL and test LoRA merges with openâ€‘source checkpoints specialised for space art.

Video finetuning: currently the generate_videos.py script uses the default Stable Video Diffusion model

. Finetuning on astronomy videos could produce smoother, domainâ€‘specific animations.

Research metrics: compute and report full FID and IS on the validation set and integrate these into the CI pipeline.

ğŸ“„ License

This project is released under the Apache 2.0 license. See LICENSE for details.
